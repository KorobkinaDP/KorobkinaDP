{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм работает очень медленно, если словарь большой, поэтому уменьшим его объем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=120,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'however', 'still', 'also', 'there', 'side', 'cannot', 'formerly', 'than', 'while', 'bill', 'besides', 'hundred', 're', 'thick', 'both', 'thin', 'those', 'three', 'whither', 'behind', 'same', 'any', 'mill', 'always', 'above', 'anything', 'often', 'our', 'me', 'latter', 'become'...e', 'thru', 'yourself', 'rather', 'noone', 'hereafter', 'together', 'beside', 'so', 'thence', 'ie'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df = 120)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем функцию, которая ранжирует элементы по их весу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_weights(weights):\n",
    "    norms = np.sort(weights) / np.sum(weights)\n",
    "    bounds = np.cumsum(norms)\n",
    "\n",
    "    rand = np.random.rand()\n",
    "    for i in range(len(weights)):\n",
    "        if(rand < bounds[i]):\n",
    "            rand = np.argsort(weights)[i]\n",
    "            break;\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagofword = np.zeros(len(vectorizer.vocabulary_), dtype = int) \n",
    "voltag = np.zeros(len(newsgroups_train.target_names))           \n",
    "numwordintag = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))                                                           \n",
    "numwordsintagtxt = np.zeros((len(newsgroups_train.data), len(newsgroups_train.target_names)))                                          \n",
    "\n",
    "alpha = np.zeros(len(newsgroups_train.target_names))         \n",
    "beta = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))  \n",
    "                                                                \n",
    "\n",
    "# случайно распределим слова по тэгам\n",
    "for i in range(len(vectorizer.vocabulary_)):       \n",
    "    tagofword[i] = range_weights(np.full(20, 1/20))\n",
    "    \n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    alpha[newsgroups_train.target[i]] = alpha[newsgroups_train.target[i]] + 1\n",
    "    doc = newsgroups_train.data[i]\n",
    "    beta[newsgroups_train.target[i]] = beta[newsgroups_train.target[i]] + vectorizer.transform([doc])\n",
    "    \n",
    "    x = np.resize(vectorizer.transform([doc]).toarray(), len(vectorizer.vocabulary_))\n",
    "    b = np.argwhere(x)\n",
    "    c = tagofword[b]\n",
    "    for j in range(len(voltag)):\n",
    "        numwordsintagtxt[i, j] = len(c[(c == j)])\n",
    "        voltag[j] = voltag[j] + len(c[(c == j)])\n",
    "    doc_transformed = vectorizer.inverse_transform(vectorizer.transform([doc]))[0]\n",
    "    for j in range(len(doc_transformed)):\n",
    "        word = vectorizer.vocabulary_.get(doc_transformed[j])\n",
    "        numwordintag[tagofword[word], word] = numwordintag[tagofword[word], word] + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in range(50):                                         \n",
    "    for i in range(len(newsgroups_train.data)):\n",
    "        doc = newsgroups_train.data[i]\n",
    "        doc_transformed = vectorizer.inverse_transform(vectorizer.transform([doc]))[0]\n",
    "        for j in range(len(doc_transformed)):\n",
    "            word = vectorizer.vocabulary_.get(doc_transformed[j])\n",
    "            tag = tagofword[word]\n",
    "            numwordsintagtxt[i, tag] = numwordsintagtxt[i, tag] - 1\n",
    "            voltag[tag] = voltag[tag] - 1\n",
    "            numwordintag[tag, word] = numwordintag[tag, word] - 1\n",
    "            #\n",
    "            p = np.zeros(len(voltag))\n",
    "            for k in range(len(voltag)):\n",
    "                p[k] = (numwordsintagtxt[i, k] + alpha[k]) * (numwordintag[k, word] + beta[k, word]) / (voltag[k] + np.sum(beta[k]))\n",
    "            tag = range_weights(np.abs(p))\n",
    "            tagofword[word] = tag\n",
    "            numwordsintagtxt[i, tag] = numwordsintagtxt[i, tag] + 1\n",
    "            voltag[tag] = voltag[tag] + 1\n",
    "            numwordintag[tag, word] = numwordintag[tag, word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic = alt.atheism\n",
      "\n",
      "know good doesn isn world ll statement stay considered \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.graphics\n",
      "\n",
      "file color called following hardware pub high sun technical \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.os.ms-windows.misc\n",
      "\n",
      "pc ftp try group 24 latest email drive mb \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.sys.ibm.pc.hardware\n",
      "\n",
      "thanks computer new board 486 hi memory data machine \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.sys.mac.hardware\n",
      "\n",
      "apple use problem video disk bit info modem chip \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.windows.x\n",
      "\n",
      "window server just display files function doing change request \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = misc.forsale\n",
      "\n",
      "shipping 30 100 included list 16 selling windows 300 \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.autos\n",
      "\n",
      "don think time did thought gas hear inside 20 \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.motorcycles\n",
      "\n",
      "like need little getting left looking miles better hand \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.sport.baseball\n",
      "\n",
      "season players win teams player pretty major week certainly \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.sport.hockey\n",
      "\n",
      "play year games played let right goal ve 14 \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.crypt\n",
      "\n",
      "using want going strong technology edu legal chips trust \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.electronics\n",
      "\n",
      "low output run things actually source say digital running \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.med\n",
      "\n",
      "years medical cause really food taking months won hope \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.space\n",
      "\n",
      "nasa idea available project day point got sci news \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = soc.religion.christian\n",
      "\n",
      "christ way bible church reason love used work probably \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.guns\n",
      "\n",
      "make government weapons country citizens makes real look american \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.mideast\n",
      "\n",
      "israel does state history attack states believe far saying \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.misc\n",
      "\n",
      "clinton long different power society order war able issues \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.religion.misc\n",
      "\n",
      "people christian read moral course matter sure book human \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# наиболее часто встречающиеся слова по конкретному тэгу\n",
    "\n",
    "InvDict = {v:k  for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    print('Topic = {0}\\n'.format(newsgroups_train.target_names[i]))\n",
    "    x = np.argsort(beta[i]) [tagofword[np.argsort(beta[i])] == i] [:-10:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(InvDict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    print()\n",
    "    print('--------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, алгоритм производит сортировку слов по тэгам довольно действенно, даже с учетом того, что было произведено мало итераций (из-за большого объема данных алгоритм работает долго, поэтому пришлось сократить их количество, чтобы получить результат быстрее). Таким образом, чтобы получить качественные результаты работы программы, нужно увеличить количество итераций в несколько раз."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
