{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм работает очень медленно, если словарь большой, поэтому уменьшим его объем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=120,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'otherwise', 'show', 'system', 'forty', 'within', 'which', 'whereas', 'am', 'someone', 'also', 'serious', 'three', 'whoever', 'made', 'the', 'nor', 'only', 'amount', 'be', 'same', 'well', 'next', 'once', 'keep', 'sincere', 'both', 'since', 'why', 'front', 'fifty', 'back', 'eith...ver', 'again', 'nothing', 'anyone', 'few', 'cry', 'anyhow', 'whither', 'it', 'yourselves', 'until'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df = 120)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем функцию, которая ранжирует элементы по их весу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_weights(weights):\n",
    "    norms = np.sort(weights) / np.sum(weights)\n",
    "    bounds = np.cumsum(norms)\n",
    "\n",
    "    rand = np.random.rand()\n",
    "    for i in range(len(weights)):\n",
    "        if(rand < bounds[i]):\n",
    "            rand = np.argsort(weights)[i]\n",
    "            break;\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagofword = np.zeros(len(vectorizer.vocabulary_), dtype = int) \n",
    "voltag = np.zeros(len(newsgroups_train.target_names))           \n",
    "numwordintag = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))                                                           \n",
    "numwordsintagtxt = np.zeros((len(newsgroups_train.data), len(newsgroups_train.target_names)))                                          \n",
    "\n",
    "alpha = np.zeros(len(newsgroups_train.target_names))         \n",
    "beta = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))  \n",
    "                                                                \n",
    "\n",
    "# случайно распределим слова по тэгам\n",
    "for i in range(len(vectorizer.vocabulary_)):       \n",
    "    tagofword[i] = range_weights(np.full(20, 1/20))\n",
    "    \n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    alpha[newsgroups_train.target[i]] = alpha[newsgroups_train.target[i]] + 1\n",
    "    doc = newsgroups_train.data[i]\n",
    "    beta[newsgroups_train.target[i]] = beta[newsgroups_train.target[i]] + vectorizer.transform([doc])\n",
    "    \n",
    "    x = np.resize(vectorizer.transform([doc]).toarray(), len(vectorizer.vocabulary_))\n",
    "    b = np.argwhere(x)\n",
    "    c = tagofword[b]\n",
    "    for j in range(len(voltag)):\n",
    "        numwordsintagtxt[i, j] = len(c[(c == j)])\n",
    "        voltag[j] = voltag[j] + len(c[(c == j)])\n",
    "    doc_transformed = vectorizer.inverse_transform(vectorizer.transform([doc]))[0]\n",
    "    for j in range(len(doc_transformed)):\n",
    "        word = vectorizer.vocabulary_.get(doc_transformed[j])\n",
    "        numwordintag[tagofword[word], word] = numwordintag[tagofword[word], word] + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in range(50):                                         \n",
    "    for i in range(len(newsgroups_train.data)):\n",
    "        doc = newsgroups_train.data[i]\n",
    "        doc_transformed = vectorizer.inverse_transform(vectorizer.transform([doc]))[0]\n",
    "        for j in range(len(doc_transformed)):\n",
    "            word = vectorizer.vocabulary_.get(doc_transformed[j])\n",
    "            tag = tagofword[word]\n",
    "            numwordsintagtxt[i, tag] = numwordsintagtxt[i, tag] - 1\n",
    "            voltag[tag] = voltag[tag] - 1\n",
    "            numwordintag[tag, word] = numwordintag[tag, word] - 1\n",
    "            #\n",
    "            p = np.zeros(len(voltag))\n",
    "            for k in range(len(voltag)):\n",
    "                p[k] = (numwordsintagtxt[i, k] + alpha[k]) * (numwordintag[k, word] + beta[k, word]) / (voltag[k] + np.sum(beta[k]))\n",
    "            tag = range_weights(np.abs(p))\n",
    "            tagofword[word] = tag\n",
    "            numwordsintagtxt[i, tag] = numwordsintagtxt[i, tag] + 1\n",
    "            voltag[tag] = voltag[tag] + 1\n",
    "            numwordintag[tag, word] = numwordintag[tag, word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic = alt.atheism\n",
      "\n",
      "don mean evidence moral mind position society got exists \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.graphics\n",
      "\n",
      "know line ll anybody video post dos algorithm running \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.os.ms-windows.misc\n",
      "\n",
      "file ms drivers win time change directory screen able \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.sys.ibm.pc.hardware\n",
      "\n",
      "bus way monitor port info possible 15 os recently \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.sys.mac.hardware\n",
      "\n",
      "mail help ram hi looking fax called let email \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.windows.x\n",
      "\n",
      "like server windows unix machine number ftp function return \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = misc.forsale\n",
      "\n",
      "used make drive software 12 address controller modem color \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.autos\n",
      "\n",
      "car year long going getting point try come case \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.motorcycles\n",
      "\n",
      "probably bad area looks hey driving quite oh bought \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.sport.baseball\n",
      "\n",
      "good best run ve won lost close red work \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.sport.hockey\n",
      "\n",
      "game play games players teams second 20 points 27 \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.crypt\n",
      "\n",
      "use encryption secure data right really technology com national \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.electronics\n",
      "\n",
      "power look radio input supply advance question design equipment \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.med\n",
      "\n",
      "think soon believe read fact research person need scientific \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.space\n",
      "\n",
      "space nasa new cost large thing information money available \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = soc.religion.christian\n",
      "\n",
      "just christ christian christians faith christianity love sure given \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.guns\n",
      "\n",
      "gun does control federal citizens makes edu children feel \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.mideast\n",
      "\n",
      "people israel jews did years jewish day took killing \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.misc\n",
      "\n",
      "say want little doing general health april trying order \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.religion.misc\n",
      "\n",
      "god said things reason live issue belief asked reference \n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# наиболее часто встречающиеся слова по конкретному тэгу\n",
    "\n",
    "InvDict = {v:k  for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    print('Topic = {0}\\n'.format(newsgroups_train.target_names[i]))\n",
    "    x = np.argsort(beta[i]) [tagofword[np.argsort(beta[i])] == i] [:-10:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(InvDict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    print()\n",
    "    print('--------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic = alt.atheism\n",
      "\n",
      "wanted window sale allow force happen various process required \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.graphics\n",
      "\n",
      "ll tell great having place heard understand team consider \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.os.ms-windows.misc\n",
      "\n",
      "time able thought file man key simply include care \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.sys.ibm.pc.hardware\n",
      "\n",
      "didn possible state 15 times note price similar single \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.sys.mac.hardware\n",
      "\n",
      "help mail called looking life start email says 100 \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = comp.windows.x\n",
      "\n",
      "course far means cause pc started hardware return job \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = misc.forsale\n",
      "\n",
      "used lot true maybe idea drive software address goes \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.autos\n",
      "\n",
      "point long case try getting later rest gets fast \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.motorcycles\n",
      "\n",
      "probably agree matter saying area wouldn hear programs mr \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.sport.baseball\n",
      "\n",
      "ve work problem best standard haven talking needs expect \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = rec.sport.hockey\n",
      "\n",
      "doesn actually real old second today version human offer \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.crypt\n",
      "\n",
      "seen data days original type 40 17 appreciated week \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.electronics\n",
      "\n",
      "program local important including turn months 000 model business \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.med\n",
      "\n",
      "using believe read example group big 14 home fine \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = sci.space\n",
      "\n",
      "new thing available card space certainly source code head \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = soc.religion.christian\n",
      "\n",
      "just sure problems given law 25 especially went graphics \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.guns\n",
      "\n",
      "bit list ask control came feel low chip children \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.mideast\n",
      "\n",
      "30 known 18 assume company display normal received air \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.politics.misc\n",
      "\n",
      "say little trying doing order general nice told posted \n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Topic = talk.religion.misc\n",
      "\n",
      "things said government reason hope 16 issue net live \n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# результат относительно счетчика из алгоритма\n",
    "\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    print('Topic = {0}'.format(newsgroups_train.target_names[i]))\n",
    "    print()\n",
    "    #x = np.argsort(num_topic_word[i])[:-10:-1]\n",
    "    x = np.argsort(numwordintag[i]) [tagofword[np.argsort(numwordintag[i])] == i] [:-10:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(InvDict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    #print(x, sep = '\\n')\n",
    "    print('--------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, алгоритм производит сортировку слов по тэгам довольно действенно, даже с учетом того, что было произведено мало итераций (из-за большого объема данных алгоритм работает долго, поэтому пришлось сократить их количество, чтобы получить результат быстрее). Но итераций явно недостотаточно для формирования распределения тэгов над словами, удовлетворящего критерию стабильности.Таким образом, чтобы получить качественные результаты работы программы, нужно увеличить количество итераций в несколько раз."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
