{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,analyzer='word', \n",
    "                             binary=True,min_df=10, max_df=.04)\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "X_train.toarray()\n",
    "#docs,words=X_train.nonzero() \n",
    "#print(X_train.shape) #(11314, 10299)\n",
    "#print(len(docs)) #it is N=480590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 20\n",
    "n_k=np.zeros(topics)\n",
    "n_dk=np.zeros(topics*X_train.shape[0]).reshape(X_train.shape[0],topics)\n",
    "n_kw=np.zeros(topics*X_train.shape[1]).reshape(topics,X_train.shape[1])\n",
    "doc, word = X_train.nonzero()\n",
    "z = np.random.choice(topics, len(doc))\n",
    "    \n",
    "for i, j, k in zip(doc, word, z):\n",
    "    n_dk[i, k] = n_dk[i, k] + 1\n",
    "    n_kw[k, j] = n_kw[k, j] + 1\n",
    "    n_k[k] = n_k[k] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(n_dk, n_kw, n_k, z, doc, word, topics, alpha, beta, it):    \n",
    "    for i in range(it):\n",
    "        for j in range(len(doc)):\n",
    "            cur_word = word[j]\n",
    "            cur_doc = doc[j]\n",
    "            cur_topic = z[j]\n",
    "            \n",
    "            n_dk[cur_doc, cur_topic] = n_dk[cur_doc, cur_topic] - 1\n",
    "            n_kw[cur_topic, cur_word] = n_kw[cur_topic, cur_word - 1\n",
    "            n_k[cur_topic] = n_k[cur_topic] - 1\n",
    "            \n",
    "            p = (n_dk[cur_doc, :] + alpha) * (n_kw[:, cur_word] + beta[cur_word]) / (n_k + beta.sum())\n",
    "            z[j] = np.random.choice(np.arange(topics), p=p / p.sum())\n",
    "            \n",
    "            n_dk[cur_doc, z[j]] = n_dk[cur_doc, z[j]] + 1\n",
    "            n_kw[z[j], cur_word] = n_kw[z[j], cur_word] + 1\n",
    "            n_k[z[j]] = n_k[z[j]] + 1\n",
    "    return z, n_kw, n_dk, n_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z,n_kw, n_dk, n_k=lda(n_dk, n_kw, n_k, z, doc, word, 20, 2*np.ones(20), 2*np.ones(X_train.shape[1]), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banks\tcadre\tchastity\tgeb\tgordon\tpitt\tshameful\tskepticism\tsoon\tsurrender\n",
      "car\tcouple\tmoney\tnet\tnice\toh\tsays\tstuff\tthank\twasn\n",
      "appreciated\tbike\tcar\tcars\tlooks\tsmall\tstuff\tweek\twonder\tworth\n",
      "anybody\tcheck\tcomes\teffect\tgoes\tleft\toh\tsimple\tsingle\twouldn\n",
      "code\tcost\tcouldn\tcouple\tdeal\thead\thear\tnice\treply\ttype\n",
      "came\tchildren\tisrael\tisraeli\tjews\tkilled\tland\ttook\twar\twomen\n",
      "1993\tapril\tcenter\tgeneral\tlow\tnational\tresearch\tscience\tspace\tuniversity\n",
      "car\tcertainly\tcouple\texactly\thome\tknows\tnice\tposting\tprice\tradio\n",
      "algorithm\tchip\tclipper\tencryption\tkey\tkeys\tphone\tpublic\tsecure\tsecurity\n",
      "cause\tchange\texperience\tfeel\tinstead\tlove\tok\twasn\twondering\twouldn\n",
      "car\tcurrent\tdifference\tguess\tmike\tmoney\tsorry\tsounds\tstuff\tthank\n",
      "card\tcomputer\tdisk\tdos\tmemory\tpc\tprice\tsale\tspeed\tvideo\n",
      "11\t12\t14\t17\t24\tgame\tgames\tplay\tseason\tteam\n",
      "advance\tanybody\tbuy\tcar\tcouple\tguy\tmentioned\tphone\trest\tservice\n",
      "aren\tdifference\thaven\thear\tleft\tmonths\topen\trunning\tsorry\twanted\n",
      "answer\tanybody\tbike\tcost\tfeel\tsorry\ttype\tunless\tusually\tvalue\n",
      "100\tanybody\tcouple\tfriend\tguess\thaven\tposting\tsimple\tsomebody\tsource\n",
      "code\tfile\tfiles\tftp\trunning\tserver\tsource\tuser\tversion\twindow\n",
      "american\tcontrol\tfederal\tgun\tguns\tlaw\tnational\tpay\tpublic\tstates\n",
      "bible\tchrist\tchristian\tchristians\tclaim\tjesus\tman\tsaying\tsays\tword\n"
     ]
    }
   ],
   "source": [
    "answer = np.argsort(n_kw, axis=1)[:, -10:]\n",
    "for i in range(20):\n",
    "    matrix = np.zeros((1, X_train.shape[1]))\n",
    "    for j in answer[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print('\\t'.join(vectorizer.inverse_transform(matrix)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы вывели топ-10 слов из заданного количества категорий (20) согласно алгоритму. Заметим, что можно дать интерпретацию большинству из групп слов.\n",
    "К некоторым категориям нельзя однозначно подобрать тэги, как и к некоторым тэгам нельзя однозначно подобрать категории. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
